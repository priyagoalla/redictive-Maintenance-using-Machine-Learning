{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Cvzqnnc9wVV_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Required Libraries Loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# **Kernel 1: Load Required Libraries**\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"✅ Required Libraries Loaded!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ex6fPaz9wdUw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Required Libraries Loaded!\n",
      "✅ Training, Sample, and Test Data Loaded Successfully!\n",
      "✅ Data Preprocessing Complete for Train, Sample, and Test Data!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"✅ Required Libraries Loaded!\")\n",
    "\n",
    "# **🔹 Kernel 2: Load Data from AWS S3**\n",
    "bucket_name = \"hackathon-predictive-maintenance\"\n",
    "\n",
    "# **File Paths for Training, Sample, and Test Data**\n",
    "train_files = [\n",
    "    \"belt_1_9_months_negative_data.xlsx\", \n",
    "    \"belt_2_9_months_negative_data.xlsx\",\n",
    "    \"belt_3_9_months_negative_data.xlsx\",\n",
    "    \"belt_4_9_months_negative_data.xlsx\",\n",
    "    \"belt_5_9_months_negative_data.xlsx\"\n",
    "]\n",
    "test_files = [\n",
    "    \"belt_1_test_data.xlsx\",\n",
    "    \"belt_2_test_data.xlsx\",\n",
    "    \"belt_3_test_data.xlsx\",\n",
    "    \"belt_4_test_data.xlsx\",\n",
    "\"belt_5_test_data.xlsx\"\n",
    "]\n",
    "\n",
    "sample_data_file = \"belt_sample_data.xlsx\"\n",
    "sample_result_file = \"belt_sample_result_sheet.xlsx\"\n",
    "\n",
    "# **Initialize S3 Client**\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# **Load Training Data**\n",
    "train_dfs = []\n",
    "for file in train_files:\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=file)\n",
    "    df = pd.read_excel(io.BytesIO(obj[\"Body\"].read()))\n",
    "    train_dfs.append(df)\n",
    "\n",
    "train_data = pd.concat(train_dfs, ignore_index=True)\n",
    "\n",
    "# **Load Sample Data**\n",
    "sample_data = pd.read_excel(io.BytesIO(s3_client.get_object(Bucket=bucket_name, Key=sample_data_file)[\"Body\"].read()))\n",
    "\n",
    "# **Expected Column Names (Since Test Data Has No Headers)**\n",
    "expected_columns = [\n",
    "    \"Name\", \"Timestamp\", \"Status\", \"Description\", \"Vibration Frequency\",\n",
    "    \"Vibration Amplitude\", \"Bearing Temperature\", \"Motor Temperature\",\n",
    "    \"Belt Load\", \"Torque\", \"Noise Levels\", \"Current and Voltage\",\n",
    "    \"Hydraulic Pressure\", \"Belt Thickness\", \"Roller Condition\"\n",
    "]\n",
    "\n",
    "# **Load Test Data & Assign Column Names**\n",
    "test_dfs = []\n",
    "for file in test_files:\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=file)\n",
    "    df = pd.read_excel(io.BytesIO(obj[\"Body\"].read()), header=None)  # No headers\n",
    "    df.columns = expected_columns  # Assign correct column names\n",
    "    test_dfs.append(df)\n",
    "\n",
    "test_data = pd.concat(test_dfs, ignore_index=True)\n",
    "\n",
    "print(\"✅ Training, Sample, and Test Data Loaded Successfully!\")\n",
    "\n",
    "# **🔹 Kernel 3: Data Preprocessing**\n",
    "# **Convert 'Timestamp' Column to Datetime**\n",
    "for df in [train_data, sample_data, test_data]:\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# **Custom Encoding for 'Name' (Belt ID)**\n",
    "name_mapping = {\n",
    "    \"Conveyor Belt 1\": 1,\n",
    "    \"Conveyor Belt 2\": 2,\n",
    "    \"Conveyor Belt 3\": 3,\n",
    "    \"Conveyor Belt 4\": 4,\n",
    "    \"Conveyor Belt 5\": 5\n",
    "}\n",
    "for df in [train_data, sample_data, test_data]:\n",
    "    df[\"Name\"] = df[\"Name\"].map(name_mapping)\n",
    "\n",
    "# **Custom Encoding for 'Status'**\n",
    "status_mapping = {\n",
    "    \"Running\": 1,\n",
    "    \"Maintenance\": 2,\n",
    "}\n",
    "for df in [train_data, sample_data, test_data]:\n",
    "    df[\"Status\"] = df[\"Status\"].map(status_mapping)\n",
    "\n",
    "# **Handle Missing Values in 'Description' & Encode it**\n",
    "label_encoder = LabelEncoder()\n",
    "for df in [train_data, sample_data, test_data]:\n",
    "    df[\"Description\"] = df[\"Description\"].astype(str).fillna(\"Unknown\")\n",
    "    df[\"Description\"] = label_encoder.fit_transform(df[\"Description\"])\n",
    "\n",
    "# **Extract Time-Based Features**\n",
    "for df in [train_data, sample_data, test_data]:\n",
    "    df[\"Hour\"] = df[\"Timestamp\"].dt.hour\n",
    "    df[\"Day\"] = df[\"Timestamp\"].dt.day\n",
    "    df[\"Month\"] = df[\"Timestamp\"].dt.month\n",
    "    df[\"Weekday\"] = df[\"Timestamp\"].dt.weekday\n",
    "\n",
    "print(\"✅ Data Preprocessing Complete for Train, Sample, and Test Data!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hp123CGUwd-9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total Anomalies in Training Data: 10592\n",
      "✅ Total Anomalies in Sample Data: 53\n",
      "✅ Total Anomalies in Test Data: 967\n"
     ]
    }
   ],
   "source": [
    "# **Kernel 4: Detect Anomalies in Train, Sample, and Test Data**\n",
    "\n",
    "def detect_anomalies(df):\n",
    "    \"\"\" Flags rows where sensor values exceed expected ranges. \"\"\"\n",
    "    return (\n",
    "        (df[\"Vibration Frequency\"] < 1490) | (df[\"Vibration Frequency\"] > 1510) |\n",
    "        (df[\"Vibration Amplitude\"] < 0.04) | (df[\"Vibration Amplitude\"] > 0.06) |\n",
    "        (df[\"Bearing Temperature\"] < 60) | (df[\"Bearing Temperature\"] > 80) |\n",
    "        (df[\"Motor Temperature\"] < 80) | (df[\"Motor Temperature\"] > 100) |\n",
    "        (df[\"Belt Load\"] < 1.0) | (df[\"Belt Load\"] > 1.4) |\n",
    "        (df[\"Torque\"] < 280) | (df[\"Torque\"] > 320) |\n",
    "        (df[\"Noise Levels\"] < 55) | (df[\"Noise Levels\"] > 65) |\n",
    "        (df[\"Current and Voltage\"] < 14) | (df[\"Current and Voltage\"] > 16) |\n",
    "        (df[\"Hydraulic Pressure\"] < 375) | (df[\"Hydraulic Pressure\"] > 385) |\n",
    "        (df[\"Belt Thickness\"] < 1.5) | (df[\"Belt Thickness\"] > 1.7) |\n",
    "        (df[\"Roller Condition\"] < 65)\n",
    "    )\n",
    "\n",
    "# **Apply Anomaly Detection to All Datasets**\n",
    "train_data[\"Anomaly\"] = detect_anomalies(train_data)\n",
    "sample_data[\"Anomaly\"] = detect_anomalies(sample_data)\n",
    "test_data[\"Anomaly\"] = detect_anomalies(test_data)  # ✅ Added Test Data Anomaly Detection\n",
    "\n",
    "# **Check the Number of Anomalies Detected**\n",
    "print(f\"✅ Total Anomalies in Training Data: {train_data['Anomaly'].sum()}\")\n",
    "print(f\"✅ Total Anomalies in Sample Data: {sample_data['Anomaly'].sum()}\")\n",
    "print(f\"✅ Total Anomalies in Test Data: {test_data['Anomaly'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vuOrinFuwhiA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Time_to_Anomaly Computed Successfully!\n",
      "   Name           Timestamp  Anomaly  Time_to_Anomaly\n",
      "0     4 2025-08-15 00:00:00        0         333900.0\n",
      "1     4 2025-08-15 00:15:00        0         333000.0\n",
      "2     4 2025-08-15 00:30:00        0         332100.0\n",
      "3     4 2025-08-15 00:45:00        0         331200.0\n",
      "4     4 2025-08-15 01:00:00        0         330300.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# **Ensure 'Anomaly' Exists and Is Integer**\n",
    "if \"Anomaly\" not in train_data.columns:\n",
    "    raise ValueError(\"❌ ERROR: 'Anomaly' column is missing. Run anomaly detection first!\")\n",
    "\n",
    "train_data[\"Anomaly\"] = train_data[\"Anomaly\"].astype(int)\n",
    "\n",
    "# **Step 1: Find First Anomaly Timestamp for Each Belt**\n",
    "first_anomaly_times = train_data[train_data[\"Anomaly\"] == 1].groupby(\"Name\")[\"Timestamp\"].min()\n",
    "\n",
    "# **Step 2: Map Each Belt's First Anomaly Timestamp**\n",
    "train_data[\"First_Anomaly_Timestamp\"] = train_data[\"Name\"].map(first_anomaly_times)\n",
    "\n",
    "# **Step 3: Compute Time to First Anomaly**\n",
    "train_data[\"Time_to_Anomaly\"] = (train_data[\"First_Anomaly_Timestamp\"] - train_data[\"Timestamp\"]).dt.total_seconds()\n",
    "\n",
    "# **Step 4: Drop Unnecessary Columns**\n",
    "train_data.drop(columns=[\"First_Anomaly_Timestamp\"], inplace=True)\n",
    "\n",
    "# **Step 5: Remove Rows Without Valid Time_to_Anomaly**\n",
    "train_data = train_data[train_data[\"Time_to_Anomaly\"].notna()]\n",
    "\n",
    "print(\"✅ Time_to_Anomaly Computed Successfully!\")\n",
    "print(train_data[[\"Name\", \"Timestamp\", \"Anomaly\", \"Time_to_Anomaly\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "c6t_R8AHwkcX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost Model Trained! MAE on Test Data: 5814.98 seconds\n"
     ]
    }
   ],
   "source": [
    "# **Kernel 6: Train XGBoost Model for Predicting Time_to_Anomaly**\n",
    "# Convert Timestamp to Numeric Feature\n",
    "train_data[\"Timestamp_Num\"] = train_data[\"Timestamp\"].astype('int64') // 10**9\n",
    "\n",
    "# Define Features & Target\n",
    "X = train_data.drop(columns=['Time_to_Anomaly', 'Timestamp'])\n",
    "y = train_data[\"Time_to_Anomaly\"]\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost Model\n",
    "xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "y_pred_test = xgb_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"✅ XGBoost Model Trained! MAE on Test Data: {mae:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Xx67Zg8lwojG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predicted Failure Dates for Sample Data:\n",
      "      Belt_ID Predicted Failure Date\n",
      "0          4       08/18/2025 18:46\n",
      "1          4       08/18/2025 19:03\n",
      "2          4       08/18/2025 19:11\n",
      "3          4       08/18/2025 19:29\n",
      "4          4       08/18/2025 19:39\n",
      "..       ...                    ...\n",
      "285        4       08/18/2025 20:39\n",
      "286        4       08/18/2025 21:00\n",
      "287        4       08/18/2025 21:17\n",
      "288        4       08/18/2025 20:01\n",
      "289        4       08/18/2025 20:02\n",
      "\n",
      "[290 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# **Kernel 7: Predict Anomaly Timestamp for Sample Data**\n",
    "# Convert Sample Timestamp to Numeric Feature\n",
    "sample_data[\"Timestamp_Num\"] = sample_data[\"Timestamp\"].astype('int64') // 10**9\n",
    "X_sample = sample_data.drop(columns=['Timestamp'])\n",
    "\n",
    "# Predict Time Until First Anomaly\n",
    "y_pred_sample = xgb_model.predict(X_sample)\n",
    "\n",
    "# Convert Predicted Seconds into DateTime\n",
    "predicted_failure_timestamps = sample_data[\"Timestamp\"] + pd.to_timedelta(y_pred_sample, unit=\"s\")\n",
    "\n",
    "# Create DataFrame with Results\n",
    "predicted_sample_failures_df = pd.DataFrame({\n",
    "    \"Belt_ID\": sample_data[\"Name\"],\n",
    "    \"Predicted Failure Date\": predicted_failure_timestamps.dt.strftime(\"%m/%d/%Y %H:%M\")\n",
    "})\n",
    "\n",
    "print(\"✅ Predicted Failure Dates for Sample Data:\\n\", predicted_sample_failures_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cPRspdzQwqon"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predicted Failure Dates for Test Data:\n",
      "       Belt_ID Predicted Failure Date\n",
      "0           4       08/19/2025 08:55\n",
      "1           4       08/19/2025 09:01\n",
      "2           4       08/19/2025 09:51\n",
      "3           4       08/19/2025 09:55\n",
      "4           4       08/19/2025 10:12\n",
      "...       ...                    ...\n",
      "7913        4       09/12/2025 23:54\n",
      "7914        4       09/13/2025 01:54\n",
      "7915        4       09/13/2025 02:00\n",
      "7916        4       09/13/2025 00:28\n",
      "7917        4       09/13/2025 01:34\n",
      "\n",
      "[7918 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# **Kernel 7: Predict Anomaly Timestamp for Test Data**\n",
    "# Convert Test Timestamp to Numeric Feature\n",
    "test_data[\"Timestamp_Num\"] = test_data[\"Timestamp\"].astype('int64') // 10**9\n",
    "X_test = test_data.drop(columns=['Timestamp'])\n",
    "\n",
    "# **Predict Time Until First Anomaly for Test Data**\n",
    "y_pred_test = xgb_model.predict(X_test)\n",
    "\n",
    "# **Convert Predicted Seconds into DateTime**\n",
    "predicted_failure_timestamps = test_data[\"Timestamp\"] + pd.to_timedelta(y_pred_test, unit=\"s\")\n",
    "\n",
    "# **Create DataFrame with Results**\n",
    "predicted_test_failures_df = pd.DataFrame({\n",
    "    \"Belt_ID\": test_data[\"Name\"],  # Belt Number\n",
    "    \"Predicted Failure Date\": predicted_failure_timestamps.dt.strftime(\"%m/%d/%Y %H:%M\")\n",
    "})\n",
    "\n",
    "# **Output Predictions**\n",
    "print(\"✅ Predicted Failure Dates for Test Data:\\n\", predicted_test_failures_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
